{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "# Exercise 15: Text Classification of Consumer Complaints\n",
    "\n",
    "In this exercise, you will try to categorize consumer complaints, based on the complaint narrative, using supervised machine learning with Support Vector Machines (SVM). You will also be able to experiment with different forms of data pre-processing to test the effects on the categorization of the text.\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "We will use a package called `sklearn` (Scikit-learn) for this lab. This package contains machine learning algorithms for Python focusing on classification, regression and clustering. If you wish, can read more about the details of `sklearn` [here](http://scikit-learn.org/stable/documentation.html) but it is not necessary for completing this lab exercise.\n",
    "\n",
    "First, let's load the relevant components we will use in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import textmining as tm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('ex15.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data into a `DataFrame` using Pandas and show the head of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complaints = pd.read_csv(\"Consumer_Complaints-sliced.tar.gz\")\n",
    "complaints.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure what this `.csv` file looks like in its raw format, you check the contents of it in a regular text editor, or by going to the Jupyter dashboard and opening it, just to get a hint of what data we will handle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the shape of the dataset. Use the code that you have learned to get the shape on `complaints` by replacing the ellipsis (`...`) below with your own code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.1.** How many complaints records are present in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records_in_complaints = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q41')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### What is a Term Document Matrix?\n",
    "\n",
    "The data set you work with consists of consumer complaints narrative (some description from a consumer about their complaint) alongside a lot of extra data about the complaints, such as when each complaint was made, which company it relates to, and some categorisations such as product or issue category.\n",
    "\n",
    "For this exercise we are interested in looking at the `Product` relating to each complaint. Each row in the dataset corresponds to a complaint. We need to start by creating a TDM that is a representation of these complaints in terms of a feature vector.\n",
    "\n",
    "We can use the `textmining` package to create a TDM and do some processing on it. We can experiment with several techniques for optimizing the input dataset and inspect the TDMs after processing.\n",
    "\n",
    "First, let's compile the corpus from the complaints records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = complaints.dropna(subset=[\"Consumer complaint narrative\"])  # drops null values\n",
    "complaints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2.** How many complaints records contain non-null narratives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_null_records_in_complaints = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q42')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming is a method where words are shortened to their morphological root. The algorithm that performs this truncation is adapted to the features of specific languages and thus it is not possible to use the same algorithm in Swedish as you would use in English. In this lab we focus on data in English.\n",
    "\n",
    "We will create three different TDMs based on a sample of the `Consumer_Complaints.csv` dataset. We use a sample initially because the inspecting and manipulating a TDM with a large input dataset easily becomes unworkable. To create samples from using the `sample()` function of a `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_complaints = complaints.sample(100)  # limits to a sample of 100 records\n",
    "sampled_complaints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get our corpus by just considering the `Consumer complaint narrative` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_corpus = sampled_complaints[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `textmining` package to create our TDM. Write some code to create our TDM `DataFrame` named `tdm_df`, and then output the head of the new `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = tm.TermDocumentMatrix()\n",
    "for complaint in sampled_corpus:\n",
    "    tdm.add_doc(complaint)\n",
    "tdm_df = tdm.to_df(cutoff=1)\n",
    "tdm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3.** How many features (terms) are present in the initial TDM generated from the sampled corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_sampled = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q43')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "`textmining` provides a function to apply stemming to a corpus of text. We can use the `tm.stem()` function to apply stemming to the complaint narrative corpus. The algorithm applied is the Porter stemming algorithm described in http://snowball.tartarus.org/algorithms/porter/stemmer.html\n",
    "\n",
    "The following line of code defines a function `stem_doc()` that takes as a parameter a document and returns the document with the individual words in the document already stemmed.\n",
    "\n",
    "    stem_doc = lambda x: ' '.join(tm.stem(x.split()))\n",
    "    \n",
    "Write some code to create a TDM `DataFrame` named `tdm_stemmed_df`, and then output the shape of the new `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stem_doc = lambda x: ' '.join(tm.stem(x.split()))\n",
    "tdm_stemmed = tm.TermDocumentMatrix()\n",
    "for complaint in sampled_corpus:    \n",
    "    tdm_stemmed.add_doc(stem_doc(complaint))\n",
    "tdm_stemmed_df = tdm_stemmed.to_df(cutoff=1)\n",
    "tdm_stemmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.4.** How many features (terms) are present in the stemmed TDM generated from the sampled corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_sampled_stemmed = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q44')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.5.** What do you observe about the shapes of `tdm_df` and `tdm_stemmed_df`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Let's now study some of the terms in `tdm_df` and` tdm_stemmed_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_stemmed = pd.concat([ \n",
    "    pd.Series(sorted(tdm_df.columns)), \n",
    "    pd.Series(sorted(tdm_stemmed_df.columns))\n",
    "], ignore_index=True, axis=1)\n",
    "compare_stemmed.columns = [\"Raw terms\", \"Stemmed terms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_stemmed[0:50]\n",
    "# you can change the selection range 20:70 to view other parts of the comparison data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.6.** How do the terms differ in a TDM with stemming from a TDM without stemming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Stopwords\n",
    "\n",
    "Stopwords are words of limited importance that do not significantly affect the text analysis. Words that are filtered out are, for example, prepositions (prepositioner) and conjunctions (konjunktioner). The default setting for using the `TermDocumentMatrix` does not remove any works when you use the `add_doc()` function.\n",
    "\n",
    "- A *preposition*  is a word that tells you where or when something is in relation to something else. For example, words like \"after\", \"before\", \"on\", \"under\", \"inside\" and \"outside\".`\n",
    "- A *conjunction* is a connective word that join sentences together. For example, the FANBOYS words: \"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\".\n",
    "\n",
    "To remove the stopwords we use the function `tm.simple_tokenize_remove_stopwords()` applied to each document to add to the TDM. What this function does is takes a document, tokenizes it (splits up the document into a list of individual words), and at the same time removes the stopwords from the list. We then reconstruct the document using `' '.join()`, which takes a list of strings and concatenates them with a space. This results in getting the document with the stopwords removed.\n",
    "\n",
    "Run the following code to apply this to the sampled corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm_stopped = tm.TermDocumentMatrix()\n",
    "for complaint in sampled_corpus:\n",
    "    complaint_stopped = ' '.join(tm.simple_tokenize_remove_stopwords(complaint))\n",
    "    tdm_stopped.add_doc(complaint_stopped)\n",
    "tdm_stopped_df = tdm_stopped.to_df(cutoff=1)\n",
    "tdm_stopped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.7.** How many features (terms) are present in the stop word truncated TDM generated from the sampled corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_sampled_stopped = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q47')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Study the terms in `tdm_df` and` tdm_stopped_df`. The following code cell creates a table from two `Series` of terms so that you can more easily compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_stopped = pd.concat([ \n",
    "    pd.Series(tdm_df.columns), \n",
    "    pd.Series(tdm_stopped_df.columns)\n",
    "], ignore_index=True, axis=1)\n",
    "compare_stopped.columns = [\"Raw terms\", \"Stopped terms\"]\n",
    "compare_stopped[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.8.** How do the terms differ in a TDM with removal of stopwords from a TDM without truncation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.9.** How does the deletion of stopwords affect the calculation efficiency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect the stopwords list used by the `textmining` package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "#### Frequency\n",
    "\n",
    "In the commands that you execute above, the meaning of words in the feature vectors is recorded based on only the number of occurences of each term in each record of the corpus (the DTM, loaded initially into `tdm_df`).\n",
    "\n",
    "Another further matrix we can derive is a TF-IDF (term frequency inverse document frequency) matrix. This emphasizes the occurrence of a word in a particular document in relation to whether the word appears in the other documents. This means that if a word occurs in almost all documents, it is allocated a lower value in the TDM. A word that appears only in a few documents is instead weighted higher. An easier way to fold a word into the feature vector is by means of TF (term frequency). TF weight the words in the feature vector in such a way that it only calculates the occurrence of the word in a document and records this in the feature vector.\n",
    "\n",
    "Instead of manually creating our TDM and then our TF-IDF matrix manually, the `sklearn` package provides some components that allow us to do this for us. Firstly, we can use a `CountVectorizer` that can take an input corpus and create an initial TDM. We then apply the `TfidfTransformer` to calculate and apply the IDF weights to the TF values. `sklearn` then allows us to train models to classify texts.\n",
    "\n",
    "Inspect the TF-IDF matrix created below with a small corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_corpus = pd.Series([\n",
    "    'She watches bandy and football', \n",
    "    'Alice likes to play bandy', \n",
    "    'Karl loves to play football'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix = count_vectorizer.fit_transform(simple_corpus)\n",
    "\n",
    "tf_df = pd.DataFrame(data=term_freq_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "tf_df.style.set_caption('Term Document Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(term_freq_matrix)\n",
    "tf_idf_matrix = tfidf.transform(term_freq_matrix)\n",
    "\n",
    "tf_idf_df = pd.DataFrame(data=tf_idf_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "tf_idf_df.style.set_caption('Term Frequency-Inverse Document Frequency Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.10.** Describe how the weighting of terms differs depending on how the frequency is calculated based on the terms found above.\n",
    "\n",
    "*You can try adding and removing to the `simple_corpus` and re-running the cells to help you observe changes in weights.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Create a Term Document Matrix\n",
    "\n",
    "Now it's time to get back to our consumer complaints data set and create TF-IDF for text analysis. In this section, you will use one of another dataset that contains. Start by clearing data and values. You will create three different TDMs and vary the input by applying stop words and then stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=1)\n",
    "term_freq_matrix = count_vectorizer.fit_transform(sampled_corpus)\n",
    "tdm_df = pd.DataFrame(data=term_freq_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "tdm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_stopped = CountVectorizer(min_df=1, stop_words=tm.stopwords)\n",
    "term_freq_matrix_stopped = count_vectorizer_stopped.fit_transform(sampled_corpus)\n",
    "tdm_stopped_df = pd.DataFrame(data=term_freq_matrix_stopped.toarray(), columns=count_vectorizer_stopped.get_feature_names())\n",
    "tdm_stopped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CountVectorizer().build_analyzer()\n",
    "def stemmed_words(doc):\n",
    "    return (tm.stem(w) for w in analyzer(doc))\n",
    "count_vectorizer_stemmed = CountVectorizer(min_df=1, analyzer=stemmed_words)\n",
    "term_freq_matrix_stemmed = count_vectorizer_stemmed.fit_transform(sampled_corpus)\n",
    "tdm_stopped_stemmed_df = pd.DataFrame(data=term_freq_matrix_stemmed.toarray(), columns=count_vectorizer_stemmed.get_feature_names())\n",
    "tdm_stopped_stemmed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_matrix(tdm, features):\n",
    "    transformer = TfidfTransformer()\n",
    "    tf_idf_matrix = transformer.fit_transform(tdm)\n",
    "    tfidf_df = pd.DataFrame(data=tf_idf_matrix.toarray(), columns=features)\n",
    "    return tfidf_df\n",
    "\n",
    "tfidf_matrix = create_tfidf_matrix(term_freq_matrix, count_vectorizer.get_feature_names())\n",
    "tfidf_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Q4.11.** What are the implications of data pre-processing for the objectivity of an analysis? (e.g. see Boyd & Crawford 2012 for a discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Training of SVM and classification\n",
    "\n",
    "When you have created a DTM, it is time to divide the data set into a training set and a test set. Classification with supervised machine learning requires a training set as the algorithm learns how to categorize data. An SVM is customized so that they can classify the training set. The classifier is then tested on the test set. In `sklearn` there is a method `train_test_split` to extract a training data set from the full data.\n",
    "\n",
    "Since training our classifier takes some time if we use the full complaints dataset, we will load the first 100000 rows only for the purposes of the rest of this lab. Run the next cell to reload the complaints dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints = pd.read_csv(\"Consumer_Complaints-sliced.tar.gz\", nrows=100000).dropna(subset=[\"Consumer complaint narrative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now visualize the distribution of complaint records according to the product categorization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "complaints.groupby('Product')[\"Consumer complaint narrative\"].count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.12.** What can you observe about the number of complaints per product? How might this affect our analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with no data pre-processing\n",
    "\n",
    "We will use the `LinearSVC` model from `sklearn` to create our classifier. We will use the `CountVectorizer` and then the `TfidfTransformer` to create an input TF-IDF matrix that we will train our model with, alongside the relevant labels.\n",
    "\n",
    "When training a model, we take an input dataset, in our case the input complaints records, and split it into a training dataset and a test dataset. This allows us to train the model with labelled data, and then test the trained model with labeled data that was not used in the training process. The `train_test_split()` function by default split the input data into 75% training data and 25% test data.\n",
    "\n",
    "Run the next cell to train the model on the input `complaints` data that we loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(complaints['Consumer complaint narrative'], \n",
    "                                                    complaints.Product, random_state=0)\n",
    "count_vectorizer = CountVectorizer(stop_words=None)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "model = LinearSVC()\n",
    "classifier = model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test out classifer against the test data `X_test`. We have to firstly vectorize the input data, then pass it to the `predict()` function of the classifier. We then build a `DataFrame` so we can inspect the predicted categories against the actual categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = count_vectorizer.transform(X_test)\n",
    "predictions = classifier.predict(vec)\n",
    "results = pd.DataFrame({\n",
    "    \"Complaint narrative\": X_test,\n",
    "    \"Actual category\": y_test,\n",
    "    \"Predicted category\": predictions\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the results table above, we can see if the classifier has done a good or bad job (it should have done an OK job). However, we can quantify the accuracy. We do this using cross-validation. `sklearn` allows us to do this with the `cross_val_score()` function.\n",
    "\n",
    "Run the next cell to run cross-validation on our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "no_pprocess_scores = cross_val_score(classifier, tdf_vectorizer.fit_transform(X_test), predictions, scoring='accuracy', cv=5)\n",
    "no_pprocess_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with truncation of stop words\n",
    "\n",
    "Next, we will create a classifier using the stopwords we used earlier from the `textmining` package, and then run the cross-validation on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(complaints['Consumer complaint narrative'], \n",
    "                                                    complaints.Product, random_state=0)\n",
    "stopped_count_vect = CountVectorizer(stop_words=tm.stopwords)\n",
    "X_train_counts = stopped_count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "stopped_classifier = LinearSVC().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "stopped_predictions = stopped_classifier.predict(stopped_count_vect.transform(X_test))\n",
    "stopped_scores = cross_val_score(stopped_classifier, tdf_vectorizer.fit_transform(X_test), stopped_predictions, scoring='accuracy', cv=5)\n",
    "stopped_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with stemming\n",
    "\n",
    "Next, we will create a classifier but apply stemming to our input data, using the stemming function we defined earlier `stem_doc`. We then run the cross-validation on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_complaints = complaints.copy()\n",
    "stemmed_complaints['Consumer complaint narrative'] = complaints['Consumer complaint narrative'].apply(stem_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(stemmed_complaints['Consumer complaint narrative'], \n",
    "                                                    stemmed_complaints.Product, random_state=0)\n",
    "stemmed_count_vect = CountVectorizer()\n",
    "X_train_counts = stemmed_count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "stemmed_classifier = LinearSVC().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "stemmed_predictions = stemmed_classifier.predict(stemmed_count_vect.transform(X_test))\n",
    "stemmed_scores = cross_val_score(stemmed_classifier, tdf_vectorizer.fit_transform(X_test), stemmed_predictions, scoring='accuracy', cv=5)\n",
    "stemmed_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of accuracies\n",
    "\n",
    "You can probably see from each of the cross-validation results the general accuracies, but to make things a little bit clearer we can build a `DataFrame` to compare these, and the visualize the results. We will do this twice, so we have written a function to builds the comparison for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_comparison(labels, scores):\n",
    "    d = {}\n",
    "    for x, y in zip(labels, scores):\n",
    "        d[x] = y\n",
    "    cross_val_scores = pd.DataFrame(d)\n",
    "    _ = cross_val_scores.boxplot().set_title(\n",
    "        \"Cross validation scores trained on {} records\".format(len(complaints)))\n",
    "    return cross_val_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the cross-validation scores for 100000 records as input to our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(complaints) > 1000:    \n",
    "    cross_val_scores_100k = cross_val_comparison(\n",
    "        ('No preprocessing', 'With stop words', 'With stemming'),\n",
    "        (no_pprocess_scores, stopped_scores, stemmed_scores))\n",
    "    cross_val_scores_100k.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.13.** What do you observe about the cross-validated accuracies using Linear SVC without pre-processed features, stop word removed features, and stemmed features? Can you explain the reasons behind your observation(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Re-run your analysis using only 1000 records from the input complaints dataset.**\n",
    "\n",
    "*Hint: You need to change the number of input rows loaded at the beginning of the [Training of SVM and classification](#Training-of-SVM-and-classification) section by setting `nrows=1000`, then re-run the code cells that come afterwards. Instead of producing the boxplot that precedes 4.13, skip that cell and run the code cell below so that you can compare the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(complaints) < 1000:\n",
    "    cross_val_scores_1k = cross_val_comparison(\n",
    "        ('No preprocessing', 'With stop words', 'With stemming'),\n",
    "        (no_pprocess_scores, stopped_scores, stemmed_scores))\n",
    "    cross_val_scores_1k.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.14.** Does this affect your previous observations? If so, provide possible reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ok.grade('q414')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Edit this cell to type your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "When you're finished with exercise 15, get one the TA or lecturer to discuss your observations.\n",
    "\n",
    "If you are running this notebook using Binder, choose **Save and Checkpoint** from the **File** menu, **rename** your notebook to add a hyphen and your initials to the notebook name e.g. `Ex15_Text_Classification_of_Consumer_Complaints-DJ`, then choose **Download as Notebook** and save it to your computer or USB stick.\n",
    "\n",
    "If you are running this notebook on your own machine, choose **Save and Checkpoint** from the **File** menu, choose **Make a copy** from the **File** menu, then **rename** your notebook to add a hyphen and your initials to the notebook name e.g. rename from `Ex15_Text_Classification_of_Consumer_Complaints-Copy1` to `Ex15_Text_Classification_of_Consumer_Complaints-DJ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "en",
    "sv"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sv",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
